# -*- coding: utf-8 -*-
"""623 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VPkf1rVQNVdsJLUS3V1n7F0rzIXG0ubL
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing Required Libraries And Dataset
import pandas as pd
df0=pd.read_csv('satisfaction.csv')
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

#Looking At Dataset 
df0.head()

#Dropping Unsueful Column
df0=df0.drop('id',1)
df0.head()

df0.shape

#Take a look at Datasets Measures of Central Tendency 
df0.describe()

#Finding about Missing Data
np.sum(df0.isnull())

#Calculating the percentage of missing data (%0.3 of arrival delay is missed)
df0.isnull().sum()/len(df0)

#Finding whether features are independent or not
plt.figure(figsize=(20,10))
corrMatrix = df0.corr()
sns.heatmap(corrMatrix,annot=True,cmap='coolwarm')
plt.show()

## Take a look at Departure/Arrival variables distribution
plt.hist(df0['Departure Delay in Minutes'])
plt.xlabel('Departure Delay in Minutes')
plt.show()

plt.hist(df0['Arrival Delay in Minutes'])
plt.xlabel('Arrival Delay in Minutes')
plt.show()

## Take a  better look at Departure/Arrival variables distribution
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(y='Departure Delay in Minutes',data=df0)
plt.figure(figsize=(16,4))
plt.subplot(1,2,2)
sns.boxplot(y='Arrival Delay in Minutes',data=df0)

#Checking the relation of Arrival and Depart Delay
sns.lmplot(x='Departure Delay in Minutes',y='Arrival Delay in Minutes',data=df0)

#Replacing missing value of Arrival Delay with zero since we believe this amount is reported in Departe Delay
df0['Arrival Delay in Minutes'] = df0['Arrival Delay in Minutes'].replace(np.nan, 0)

#Joing the information of Arrival & Departure Delay in a single column to prevent dependency of featurears
df0['Real Delay In Minutes']=df0['Arrival Delay in Minutes']+abs(df0['Arrival Delay in Minutes']-df0['Departure Delay in Minutes'])
df0=df0.drop(['Arrival Delay in Minutes','Departure Delay in Minutes'],axis=1)

#Finding about maximum Real delay variables
df0.sort_values(by=['Real Delay In Minutes'], inplace=True, ascending=False)
df0.head(10)

df0.describe()

#Checking percent of outliers
print(np.sum(df0['Flight Distance']>5359))
print(np.sum(df0['Flight Distance']>5359)/len(df0))

print(np.sum(df0['Real Delay In Minutes']>88))
print(np.sum(df0['Real Delay In Minutes']>88)/len(df0))
print(np.sum(df0['Real Delay In Minutes']>149))
print(np.sum(df0['Real Delay In Minutes']>149)/len(df0))

#Converting all categorical features  to  numeric variables
def Delay_Check(x):
  if x >= 180:
    return 180
  else:
    return x
df0['Real Delay In Minutes']=df0['Real Delay In Minutes'].apply(Delay_Check)

print(np.sum(df0['Real Delay In Minutes']>75))
print(np.sum(df0['Real Delay In Minutes']>75)/len(df0))

df0.sort_values(by=['Real Delay In Minutes'], inplace=True, ascending=False)
df0.head()

df0.describe()

#Finding the frequency and percentage of categorical variables 
print(df0['satisfaction_v2'].value_counts())
print(df0['satisfaction_v2'].value_counts()/len(df0))

print(df0['Gender'].value_counts())
print(df0['Gender'].value_counts()/len(df0))

print(df0['Customer Type'].value_counts())
print(df0['Customer Type'].value_counts()/len(df0))

print(df0['Type of Travel'].value_counts())
print(df0['Type of Travel'].value_counts()/len(df0))

print(df0['Class'].value_counts())
print(df0['Class'].value_counts()/len(df0))

## Take a look at Class Variable
plt.hist(df0['satisfaction_v2'])
plt.xlabel('satisfaction_v2')
plt.show()

## Take a look at Fact Features distribution
plt.hist(df0['Gender'])
plt.xlabel('Gender')
plt.show()

plt.hist(df0['Customer Type'])
plt.xlabel('Customer Type')
plt.show()

plt.hist(df0['Type of Travel'])
plt.xlabel('Type of Travel')
plt.show()

plt.hist(df0['Class'])
plt.xlabel('Class')
plt.show()

plt.hist(df0['Age'])
plt.xlabel('Age')
plt.show()

## Take a look at Flight Features distributions
plt.hist(df0['Flight Distance'])
plt.xlabel('Flight Distance')
plt.show()

plt.hist(df0['Real Delay In Minutes'])
plt.xlabel('Real Delay In Minutes')
plt.show()

## Take a look at Respond Features distributions
plt.hist(df0['Seat comfort'])
plt.xlabel('Seat comfort')
plt.show()

plt.hist(df0['Departure/Arrival time convenient'])
plt.xlabel('Departure/Arrival time convenient')
plt.show()

plt.hist(df0['Food and drink'])
plt.xlabel('Food and drink')
plt.show()

plt.hist(df0['Gate location'])
plt.xlabel('Gate location')
plt.show()

plt.hist(df0['Inflight wifi service'])
plt.xlabel('Inflight wifi service')
plt.show()

plt.hist(df0['Inflight entertainment'])
plt.xlabel('Inflight entertainment')
plt.show()

plt.hist(df0['Online support'])
plt.xlabel('Online support')
plt.show()

plt.hist(df0['Ease of Online booking'])
plt.xlabel('Ease of Online booking')
plt.show()

plt.hist(df0['On-board service'])
plt.xlabel('On-board service')
plt.show()

plt.hist(df0['Leg room service'])
plt.xlabel('Leg room service')
plt.show()

plt.hist(df0['Baggage handling'])
plt.xlabel('Baggage handling')
plt.show()

plt.hist(df0['Checkin service'])
plt.xlabel('Checkin service')
plt.show()

plt.hist(df0['Cleanliness'])
plt.xlabel('Cleanliness')
plt.show()

plt.hist(df0['Online boarding'])
plt.xlabel('Online boarding')
plt.show()

#Take a look at  Flight Features with  Satisfaction

#Real Delay vs Satisfaction
#Flight Distance vs Satisfaction

plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(x='satisfaction_v2', y='Real Delay In Minutes',data=df0)

plt.subplot(1,2,2)
sns.boxplot(x='satisfaction_v2', y='Flight Distance',data=df0)

s=['satisfied','neutral or dissatisfied']
plt.figure(figsize=(8,4))
for i in s:
  m=df0[df0['satisfaction_v2']==i]
  sns.distplot(m['Real Delay In Minutes'], kde=True,hist=False,kde_kws={'shade':True,'linewidth':3},label=i)

s=['satisfied','neutral or dissatisfied']
plt.figure(figsize=(8,4))
for i in s:
  N=df0[df0['satisfaction_v2']==i]
  sns.distplot(N['Flight Distance'], kde=True,hist=False,kde_kws={'shade':True,'linewidth':3},label=i)

#Take a look at Fact features with Satisfaction
# Gender vs Satisfaction
ax=sns.catplot(x='satisfaction_v2',kind='count',data=df0, hue='Gender')
ax.set(xlabel="Satisfaction", ylabel = "Count")
pd.crosstab(df0.Gender, df0.satisfaction_v2,margins=True,rownames=['Gender'],colnames=['Satisfaction']).apply(lambda r: r/len(df0))

# Customer Type vs Satisfaction
ax_2=sns.catplot(x='satisfaction_v2',kind='count',data=df0, hue='Customer Type')
ax_2.set(xlabel="Satisfaction", ylabel = "Count")
pd.crosstab(df0['Customer Type'], df0.satisfaction_v2,margins=True,rownames=['Customer Type'],colnames=['Satisfaction']).apply(lambda r: r/len(df0))

# Travel Type vs Satisfaction
sns.catplot(x='satisfaction_v2',kind='count',data=df0, hue='Type of Travel')
pd.crosstab(df0['Type of Travel'] , df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

# Class vs Satisfaction
sns.catplot(x='satisfaction_v2',kind='count',data=df0, hue='Class')
pd.crosstab(df0.Class, df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Age vs Satisfaction
sns.boxplot(x='satisfaction_v2', y='Age',data=df0)

sns.catplot("Age", data=df0, aspect=3.0, kind='count', hue='satisfaction_v2', order=range(5, 80))

#Look at each Respond features with Satisfaction
#Seat Comfort vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(x='satisfaction_v2', y='Seat comfort',data=df0)
pd.crosstab(df0['Seat comfort'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Departure/Arrival time convenient vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,2)
ax_5=sns.boxplot(x='satisfaction_v2', y='Departure/Arrival time convenient',data=df0)
ax_5.set(xlabel="Satisfaction", ylabel = "Count")
pd.crosstab(df0['Departure/Arrival time convenient'], df0.satisfaction_v2,margins=True,colnames=['Satisfaction']).apply(lambda r: r/len(df0))

#Food and drink vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
ax_4=sns.boxplot(x='satisfaction_v2', y='Food and drink',data=df0)
ax_4.set(xlabel="Satisfaction", ylabel = "Count")
pd.crosstab(df0['Food and drink'], df0.satisfaction_v2,margins=True,colnames=['Satisfaction']).apply(lambda r: r/len(df0))

#Gate location vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(x='satisfaction_v2', y='Gate location',data=df0)
pd.crosstab(df0['Gate location'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Inflight wifi service vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,2)
sns.boxplot(x='satisfaction_v2', y='Inflight wifi service',data=df0)
pd.crosstab(df0['Inflight wifi service'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Inflight entertainment vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
ax_6=sns.boxplot(x='satisfaction_v2', y='Inflight entertainment',data=df0)
ax_6.set(xlabel="Satisfaction", ylabel = "Count")
pd.crosstab(df0['Inflight entertainment'], df0.satisfaction_v2,margins=True,colnames=['Satisfaction']).apply(lambda r: r/len(df0))

#Online support vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,2)
ax_3=sns.boxplot(x='satisfaction_v2', y='Online support',data=df0)
ax_3.set(xlabel="Satisfaction", ylabel = "Count")
pd.crosstab(df0['Online support'], df0.satisfaction_v2,margins=True,colnames=['Satisfaction']).apply(lambda r: r/len(df0))

#Ease of Online booking vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(x='satisfaction_v2', y='Ease of Online booking',data=df0)
pd.crosstab(df0['Ease of Online booking'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#On-board service vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,2)
sns.boxplot(x='satisfaction_v2', y='On-board service',data=df0)
pd.crosstab(df0['On-board service'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Leg room service vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(x='satisfaction_v2', y='Leg room service',data=df0)
pd.crosstab(df0['Leg room service'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Baggage handling vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,2)
sns.boxplot(x='satisfaction_v2', y='Baggage handling',data=df0)
pd.crosstab(df0['Baggage handling'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Checkin service vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(x='satisfaction_v2', y='Checkin service',data=df0)
pd.crosstab(df0['Checkin service'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Cleanliness vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,2)
sns.boxplot(x='satisfaction_v2', y='Cleanliness',data=df0)
pd.crosstab(df0['Cleanliness'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

#Online boarding vs Satisfaction
plt.figure(figsize=(16,4))
plt.subplot(1,2,1)
sns.boxplot(x='satisfaction_v2', y='Online boarding',data=df0)
pd.crosstab(df0['Online boarding'], df0.satisfaction_v2,margins=True).apply(lambda r: r/len(df0))

df1=df0

#Converting class variable to a numeric variable
def Satisfaction_Check(x):
  if x =='satisfied':
    return 1
  else:
    return 0
df1['Satisfaction']=df1['satisfaction_v2'].apply(Satisfaction_Check)
df1=df1.drop(['satisfaction_v2',],axis=1)

#Converting all categorical features  to  numeric variables
def Customer_Check(x):
  if x =='Loyal Customer':
    return 1
  else:
    return 0
df1['Customer_Type']=df1['Customer Type'].apply(Customer_Check)
df1=df1.drop(['Customer Type',],axis=1)

def Gender_Check(x):
  if x =='Female':
    return 1
  else:
    return 0
df1['Gender_Type']=df1['Gender'].apply(Gender_Check)
df1=df1.drop(['Gender',],axis=1)

def Travel_Check(x):
  if x =='Business travel':
    return 1
  else:
    return 0
df1['Travel_Type']=df1['Type of Travel'].apply(Travel_Check)
df1=df1.drop(['Type of Travel',],axis=1)

def Class_Check(x):
  if x =='Business':
    return 1
  else:
    return 0
df1['Class_Type']=df1['Class'].apply(Class_Check)
df1=df1.drop(['Class',],axis=1)

#Checking 'class' relation with satisfaction after making eco and eco plus one grooup
sns.catplot(x='Satisfaction',kind='count',data=df1, hue='Class_Type')
pd.crosstab(df1.Class_Type, df1.Satisfaction,margins=True).apply(lambda r: r/len(df1))

#Rearranging the order of variables in table
df1=df1[['Satisfaction','Gender_Type','Customer_Type','Travel_Type','Class_Type','Age','Flight Distance','Real Delay In Minutes','Seat comfort','Departure/Arrival time convenient','Food and drink','Gate location','Inflight wifi service','Inflight entertainment','Online support','Ease of Online booking','On-board service','Leg room service','Baggage handling','Checkin service','Cleanliness','Online boarding']]

#Take a look at all variables to find whether all are numeric or not
df1.head()

df1.describe()

#Finding about centarl tendency of measurements for different values of class
df1.groupby('Satisfaction').mean()

#Finding about centarl tendency of measurements for different values of class
df1.groupby('Satisfaction').min()
df1.groupby('Satisfaction').max()
df1.groupby('Satisfaction').std()

#Checking the relations(correlation) of features with satisfaction
plt.figure(figsize=(20,15))
corrMatrix2 = df1.corr()
mask = np.triu(np.ones_like(corrMatrix2, dtype=np.bool))
sns.heatmap(corrMatrix2, annot=True,cmap='coolwarm', mask=mask, vmax=None, center=0,square=True,linewidths=0.6, cbar_kws={"shrink": .9})
plt.show()

#Checking the relations of features with satisfaction
corr_dia=df1.corr()['Satisfaction'].sort_values().drop('Satisfaction').plot(kind='bar')
corr_dia

df1.shape

#Importing Libraries for train/test split
from sklearn.model_selection import train_test_split
# For evaluating our ML results
from sklearn import metrics

#Defining features(X) and class variable(Y) for All Variables
Y=df1['Satisfaction']
X=df1.drop(['Satisfaction'],axis=1)

X.head()

Y.head()

# Split  train/test data for All variables
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.30)

#Seprating Respond Features 
df_Q=pd.DataFrame()

df_Q=df1[['Satisfaction','Seat comfort','Departure/Arrival time convenient','Food and drink','Gate location','Inflight wifi service','Inflight entertainment','Online support','Ease of Online booking','On-board service','Leg room service','Baggage handling','Checkin service','Cleanliness','Online boarding']]

df_Q.head()

#Defining features(X) and class variable(Y) for Respond features
Y_Q=df_Q['Satisfaction']
X_Q=df_Q.drop(['Satisfaction'],axis=1)

# Split train/test data for Respond features
X_Q_train, X_Q_test, Y_Q_train, Y_Q_test = train_test_split(X_Q,Y_Q,test_size=0.30)

#Seprating High correlated Features 

df_C=pd.DataFrame()

df_C=df1[['Satisfaction','Gender_Type','Customer_Type','Class_Type','Seat comfort','Inflight wifi service','Inflight entertainment','Online support','Ease of Online booking','On-board service','Leg room service','Baggage handling','Checkin service','Cleanliness','Online boarding']]

df_C.head()

#Defining features(X) and class variable(Y) for high correlated features
Y_C=df_C['Satisfaction']
X_C=df_C.drop(['Satisfaction'],axis=1)

# Split train/test data for High correlated features
X_C_train, X_C_test, Y_C_train, Y_C_test = train_test_split(X_C,Y_C,test_size=0.30)

# Machine Learning Imports
from sklearn.linear_model import LogisticRegression

# Make a log_model For All Variables
log_model = LogisticRegression()

# Now fit the new model
log_model.fit(X_train,Y_train)

# Predict the classes of the testing data set
class_predict_log = log_model.predict(X_test)


# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_test,class_predict_log))
print('Report', metrics.classification_report(Y_test,class_predict_log))

# Use zip to bring the column names and the np.transpose function to bring together the coefficients from the model
coeff_df = pd.DataFrame(zip(X_train.columns,np.transpose(log_model.coef_)))
coeff_df.columns=['Features','Coefficient']
coeff_df.sort_values(by='Coefficient', ascending=False)

# Make a log_model for Respond features
log_model_Q= LogisticRegression()

# Now fit the new model for  Respond features
log_model_Q.fit(X_Q_train,Y_Q_train)

# Predict the classes of the testing data set
class_predict_log_Q= log_model_Q.predict(X_Q_test)


# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_Q_test,class_predict_log_Q))
print('Report', metrics.classification_report(Y_Q_test,class_predict_log_Q))

# Use zip to bring the column names and the np.transpose function to bring together the coefficients from the model
coeff_df_Q = pd.DataFrame(zip(X_Q_train.columns,np.transpose(log_model_Q.coef_)))
coeff_df_Q.columns=['Features','Coefficient']
coeff_df_Q.sort_values(by='Coefficient', ascending=False)

# Make a log_model for High correlated  features
log_model_C= LogisticRegression()

# Now fit the new model for High correlated features
log_model_C.fit(X_C_train,Y_C_train)

# Predict the classes of the testing data set
class_predict_log_C= log_model_C.predict(X_C_test)


# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_C_test,class_predict_log_C))
print('Report', metrics.classification_report(Y_C_test,class_predict_log_C))

# Use zip to bring the column names and the np.transpose function to bring together the coefficients from the model
coeff_df_C = pd.DataFrame(zip(X_C_train.columns,np.transpose(log_model_C.coef_)))
coeff_df_C.columns=['Features','Coefficient']
coeff_df_C.sort_values(by='Coefficient', ascending=False)

# Machine Learning Imports
from sklearn.naive_bayes import GaussianNB

# Make a NaiveBayes_model For All Variables
naive_bayes = GaussianNB()

# Now fit the new model for High correlated features
naive_bayes.fit(X_train,Y_train)

# Predict the classes of the testing data set
class_predict_naive = naive_bayes.predict(X_test)


# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_test,class_predict_naive))
print('Report', metrics.classification_report(Y_test,class_predict_naive))

# Make a NaiveBayes_model For Respond Features
naive_bayes_Q = GaussianNB()

# Now fit the new model for Respond Features
naive_bayes_Q.fit(X_Q_train,Y_Q_train)

# Predict the classes of the testing data set
class_predict_naive_Q = naive_bayes_Q.predict(X_Q_test)


# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_Q_test,class_predict_naive_Q))
print('Report', metrics.classification_report(Y_Q_test,class_predict_naive_Q))

# Make a NaiveBayes_model For High Correlated Features
naive_bayes_C= GaussianNB()

# Now fit the new model for High Correlated Features
naive_bayes_C.fit(X_C_train,Y_C_train)

# Predict the classes of the testing data set
class_predict_naive_C= naive_bayes_C.predict(X_C_test)

# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_C_test,class_predict_naive_C))
print('Report', metrics.classification_report(Y_C_test,class_predict_naive_C))

# Machine Learning Imports
from sklearn.neighbors import KNeighborsClassifier

# Make a KNeighbors_model For All Variables
KNeighbors_model=KNeighborsClassifier(n_neighbors=3)

# Now fit the new model for All Variables
KNeighbors_model.fit(X_train,Y_train)

# Predict the classes of the testing data set
class_predict_KNeighbors=KNeighbors_model.predict(X_test)

# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_test,class_predict_KNeighbors))
print('Report', metrics.classification_report(Y_test,class_predict_KNeighbors))

# Make a KNeighbors_model For Respond Features
KNeighbors_model_Q=KNeighborsClassifier(n_neighbors=3)

# Now fit the new model for Respond Features
KNeighbors_model_Q.fit(X_Q_train,Y_Q_train)

# Predict the classes of the testing data set
class_predict_KNeighbors_Q=KNeighbors_model_Q.predict(X_Q_test)

# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_Q_test,class_predict_KNeighbors_Q))
print('Report', metrics.classification_report(Y_Q_test,class_predict_KNeighbors_Q))

# Make a KNeighbors_model For High Correlated Features
KNeighbors_model_C=KNeighborsClassifier(n_neighbors=3)

# Now fit the new model for High Correlated Features
KNeighbors_model_C.fit(X_C_train,Y_C_train)

# Predict the classes of the testing data set
class_predict_KNeighbors_C=KNeighbors_model_C.predict(X_C_test)

# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_C_test,class_predict_KNeighbors_C))
print('Report', metrics.classification_report(Y_C_test,class_predict_KNeighbors_C))

# Machine Learning Imports
from sklearn.tree import DecisionTreeClassifier

# Make a DecisionTree_model For All Variables
DecisionTree_model=DecisionTreeClassifier(min_samples_split=100)

# Now fit the new model for For All Variables
DecisionTree_model.fit(X_train,Y_train)

# Predict the classes of the testing data set
class_predict_DecisionTree=DecisionTree_model.predict(X_test)

# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_test,class_predict_DecisionTree))
print('Report', metrics.classification_report(Y_test,class_predict_DecisionTree))

#Graphing Decision Tree
from sklearn import tree
import graphviz
from sklearn.tree import export_graphviz
tree.plot_tree(DecisionTree_model)
treedata=tree.export_graphviz(DecisionTree_model,out_file=None)
n=graphviz.Source(treedata)
n.render("Retail")

# Make a DecisionTree_model For Respond Features
DecisionTree_model_Q=DecisionTreeClassifier(min_samples_split=100)

# Now fit the new model for Respond Features
DecisionTree_model_Q.fit(X_Q_train,Y_Q_train)

# Predict the classes of the testing data set
class_predict_DecisionTree_Q=DecisionTree_model_Q.predict(X_Q_test)

# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_Q_test,class_predict_DecisionTree_Q))
print('Report', metrics.classification_report(Y_Q_test,class_predict_DecisionTree_Q))

# Make a DecisionTree_model For High Correlated Features
DecisionTree_model_C=DecisionTreeClassifier(min_samples_split=100)

# Now fit the new model For High Correlated Variables
DecisionTree_model_C.fit(X_C_train,Y_C_train)

# Predict the classes of the testing data set
class_predict_DecisionTree_C=DecisionTree_model_C.predict(X_C_test)

# Compare the predicted classes to the actual test classes
print('Accuracy', metrics.accuracy_score(Y_C_test,class_predict_DecisionTree_C))
print('Report', metrics.classification_report(Y_C_test,class_predict_DecisionTree_C))

#Importing Roc-Curve metric
#Defining function for graphing Roc-Curve
from sklearn.metrics import  roc_curve
def graph(Model, xtest, ytest ,Label):
  y_probable=Model.predict_proba(xtest)
  y_probable=y_probable[:,1]
  fpr, tpr, _ = roc_curve(ytest, y_probable)
  plt.plot(fpr, tpr, marker='.', label=Label)

#Graphing Roc-Curve For All Variables
plt.figure(figsize=(4,4))
graph(Model=log_model, xtest=X_test ,ytest=Y_test,Label='LogisticRegression' )
graph(Model=naive_bayes, xtest=X_test ,ytest=Y_test,Label='NaiveBayes' )
graph(Model=KNeighbors_model, xtest=X_test ,ytest=Y_test,Label='KNeighbors' )
graph(Model=DecisionTree_model, xtest=X_test ,ytest=Y_test,Label='DecisionTree' )
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('All Variables Roc Curve')
# show the legend
plt.legend()
# show the plot
plt.show()

#Graphing Roc-Curve For Respond Features
plt.figure(figsize=(4,4))
graph(Model=log_model_Q, xtest=X_Q_test ,ytest=Y_Q_test,Label='LogisticRegression' )
graph(Model=naive_bayes_Q, xtest=X_Q_test ,ytest=Y_Q_test,Label='NaiveBayes' )
graph(Model=KNeighbors_model_Q, xtest=X_Q_test ,ytest=Y_Q_test,Label='KNeighbors' )
graph(Model=DecisionTree_model_Q, xtest=X_Q_test ,ytest=Y_Q_test,Label='DecisionTree' )
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Respond Features Roc Curve')
# show the legend
plt.legend()
# show the plot
plt.show()

#Graphing Roc-Curve For High Correlated Features
plt.figure(figsize=(4,4))
graph(Model=log_model_C, xtest=X_C_test ,ytest=Y_C_test,Label='LogisticRegression' )
graph(Model=naive_bayes_C, xtest=X_C_test ,ytest=Y_C_test,Label='NaiveBayes' )
graph(Model=KNeighbors_model_C, xtest=X_C_test ,ytest=Y_C_test,Label='KNeighbors' )
graph(Model=DecisionTree_model_C, xtest=X_C_test ,ytest=Y_C_test,Label='DecisionTree' )
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('High Correlated Features Roc Curve')
# show the legend
plt.legend()
# show the plot
plt.show()

# Import Libraries for Clustering
import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import scale

#Clustering Numeric variables

#Flight Distance - Real Delay
Distance_Delay=pd.concat([df1['Flight Distance'],df1['Real Delay In Minutes']],axis=1)
Distance_Delay_Clusters= scale(Distance_Delay)

#Elbow method
distortations = {}
for k in range(1,10):
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(Distance_Delay_Clusters)
  distortations[k] = kmeans.inertia_

plt.plot(list(distortations.keys()),list(distortations.values()))
plt.title('Elbow method on Airline dataset')
plt.xlabel('Number of clusters')
plt.ylabel('Within-cluster SSE')
plt.show()

clustering  = KMeans(n_clusters=3)
clustering.fit(Distance_Delay_Clusters)

#color_theme = np.array(['darkgray','lightsalmon','powderblue'])
plt.figure(figsize=(16,6))
#plt.subplot(1,2,1)
#plt.scatter(x=df1['Flight Distance'],y=df1['Real Delay In Minutes'],c=[df1['Satisfaction']],s=50)
#plt.xlabel('Flight Distance')
#plt.ylabel('Real Delay In Minutes')
#plt.title('Ground Truth Classification')

plt.subplot(1,2,2)
plt.scatter(x=df1['Flight Distance'],y=df1['Real Delay In Minutes'],c=[clustering.labels_],alpha=0.6,s=50)
plt.xlabel('Flight Distance')
plt.ylabel('Real Delay In Minutes')
plt.title('KMeans Clustering')

#Age - Real Delay

Age_Delay=pd.concat([df1['Age'],df1['Real Delay In Minutes']],axis=1)
Age_Delay_Clusters= scale(Age_Delay)

#Elbow method
distortations = {}
for k in range(1,10):
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(Age_Delay_Clusters)
  distortations[k] = kmeans.inertia_

plt.plot(list(distortations.keys()),list(distortations.values()))
plt.title('Elbow method on Airline dataset')
plt.xlabel('Number of clusters')
plt.ylabel('Within-cluster SSE')
plt.show()

clustering  = KMeans(n_clusters=3)
clustering.fit(Age_Delay_Clusters)

#color_theme = np.array(['darkgray','lightsalmon','powderblue'])
plt.figure(figsize=(16,6))

plt.subplot(1,2,1)
plt.scatter(x=df1['Age'],y=df1['Real Delay In Minutes'],c=[df1['Satisfaction']],s=50)
plt.title('Ground Truth Classification')

plt.subplot(1,2,2)
plt.scatter(x=df1['Age'],y=df1['Real Delay In Minutes'],c=[clustering.labels_],s=50)
plt.title('K-Means Classification')

#Age- Flight Distance

Age_Distance=pd.concat([df1['Age'],df1['Flight Distance']],axis=1)
Age_Distance_Clusters= scale(Age_Distance)

#Elbow method
distortations = {}
for k in range(1,10):
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(Age_Distance_Clusters)
  distortations[k] = kmeans.inertia_

plt.plot(list(distortations.keys()),list(distortations.values()))
plt.title('Elbow method on Airline dataset')
plt.xlabel('Number of clusters')
plt.ylabel('Within-cluster SSE')
plt.show()

clustering  = KMeans(n_clusters=3)
clustering.fit(Age_Distance_Clusters)

#color_theme = np.array(['darkgray','lightsalmon','powderblue'])
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.scatter(x=df1['Age'],y=df1['Flight Distance'],c=[df1['Satisfaction']],s=50)
plt.title('Ground Truth Classification')

plt.subplot(1,2,2)
plt.scatter(x=df1['Age'],y=df1['Flight Distance'],c=[clustering.labels_],s=50)
plt.title('K-Means Classification')

df2=pd.DataFrame()

#Finding Association Rules Among  Respond Features
def Transform(x):
  if x >=4:
    return True
  else:
    return False
df2['SeatComfort']=df1['Seat comfort'].apply(Transform)
df2['Departure/Arrival time convenient']=df1['Departure/Arrival time convenient'].apply(Transform)
df2['Food and drink']=df1['Food and drink'].apply(Transform)
df2['Gate location']=df1['Gate location'].apply(Transform)
df2['Inflight wifi service']=df1['Inflight wifi service'].apply(Transform)
df2['Inflight entertainment']=df1['Inflight entertainment'].apply(Transform)
df2['Online support']=df1['Online support'].apply(Transform)
df2['Ease of Online booking']=df1['Ease of Online booking'].apply(Transform)
df2['On-board service']=df1['On-board service'].apply(Transform)
df2['Leg room service']=df1['Leg room service'].apply(Transform)
df2['Baggage handling']=df1['Baggage handling'].apply(Transform)
df2['Checkin service']=df1['Checkin service'].apply(Transform)
df2['Cleanliness']=df1['Cleanliness'].apply(Transform)
df2['Online boarding']=df1['Online boarding'].apply(Transform)

df2.head()

#Import Libraries
from mlxtend.frequent_patterns import apriori, association_rules

#Defining Support level
frequent_itemsets = apriori(df2,min_support=0.45,use_colnames=True)
print(frequent_itemsets)

Rules=pd.DataFrame()

#Defining Confidence Level
Rules = association_rules(frequent_itemsets,metric='confidence',min_threshold=0.7)
Rules_Head=Rules[['antecedents','consequents','support','confidence']]

Rules_Head.sort_values(by=['confidence'], inplace=True, ascending=False)
Rules_Head